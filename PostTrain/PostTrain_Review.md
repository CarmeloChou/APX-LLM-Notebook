# PostTrain概览

| 类别                  | 核心目标                                                     | 关键方法/操作              | 典型代表/技术                                      | 作用与特点                                                   |
| :-------------------- | :----------------------------------------------------------- | :------------------------- | :------------------------------------------------- | :----------------------------------------------------------- |
| **1. 有监督微调**     | 让模型学会**遵循指令**，并适应特定任务或对话格式。           | **指令微调**               | FLAN-T5, InstructGPT (Step1), ChatGLM-SFT          | **基础适应性训练**。使用高质量的（指令，输出）配对数据，教模型理解并执行人类指令。是后续对齐的基础。 |
|                       |                                                              | **领域适应/继续预训练**    | 在特定领域（医学、法律、代码）数据上继续训练模型。 | 将通用模型“浸泡”在特定领域知识中，提升其专业能力。需注意灾难性遗忘问题。 |
| **2. 人类偏好对齐**   | 让模型的行为符合人类的**价值偏好**（有用、诚实、无害）。     | **基于人类反馈的强化学习** | InstructGPT (Step2&3), ChatGPT, Claude             | **当前效果最佳、最主流**。通过人类对模型输出的偏好排序，训练“奖励模型”，再用RL驱动模型学习人类偏好。效果好，但复杂昂贵。 |
|                       |                                                              | **从人类偏好中直接学习**   | DPO, KTO, ORPO, IPO                                | **RLHF的简化与替代方案**。直接在偏好数据上优化模型，无需训练单独的奖励模型，更稳定、高效。 |
|                       |                                                              | **宪法AI**                 | Anthropic Claude系列                               | **用原则（宪法）指导对齐**。模型生成结果后，根据一套“宪法”原则进行自我批判和修正，减少对主观人类反馈的依赖。 |
| **3. 安全与对齐增强** | 在SFT和对齐基础上，**进一步降低模型的有害输出风险**。        | **红队攻击**               | 组建“红队”模拟恶意用户，主动攻击模型以发现风险。   | **主动防御**。用于发现模型的安全漏洞和偏见，生成的数据可用于后续增强训练。 |
|                       |                                                              | **对抗性训练**             | 将红队发现的有害数据加入训练集，增强模型鲁棒性。   | 让模型“见多识广”，在被恶意诱导时仍能保持安全。               |
|                       |                                                              | **安全微调**               | 使用明确的安全-有害对话对进行训练。                | 直接、明确地教会模型拒绝不当请求。                           |
| **4. 上下文学习增强** | 在不更新模型参数的情况下，**提升模型在特定任务上的即时表现**。 | **思维链**                 | 在Prompt中要求模型展示推理步骤。                   | 解锁模型的推理能力，显著提升复杂问题（如数学、推理）的准确性。 |
|                       |                                                              | **少样本示例**             | 在Prompt中提供任务的输入输出示例。                 | 让模型快速理解任务格式和需求，是使用大模型的基础技巧。       |
| **5. 模型优化与压缩** | 让大模型能**更高效、低成本地部署和应用**。                   | **量化**                   | GPTQ, AWQ, GGUF/llama.cpp                          | 降低模型权重精度（如从FP16到INT4），大幅减少内存占用和计算需求，对精度损失较小。 |
|                       |                                                              | **知识蒸馏**               | 训练一个更小的“学生模型”来模仿大“教师模型”的行为。 | 获得一个更小、更快、保留大部分能力的模型，便于部署。         |
|                       |                                                              | **剪枝**                   | 移除模型中不重要的连接或参数。                     | 简化模型结构，降低计算量。                                   |
|                       |                                                              | **参数高效微调**           | LoRA, QLoRA, Prefix-tuning, P-tuning               | **后训练的核心技术**。只训练极少量（<1%）的额外参数，即可让模型适配新任务。效率极高，是个人和学术界微调大模型的利器。 |