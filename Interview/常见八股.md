# 百面大模型

## 0.1 语义表达

## 0.2 大模型的数据
## 0.3 大模型的与训练
## 0.4 大模型的对齐
## 0.5 大模型的垂类微调
## 0.6 大模型的组件
## 0.7 大模型的评估
## 0.8 大模型的架构
## 0.9 检索增强生成
### RAG的组成与评估
- 目的：减少幻觉问题
- 框架：LlamaIndex、LangChain
- 流程：
1. 查询改写阶段（召回前预处理）
2. 召回阶段。一般使用稀疏的 TF—IDF 召回或者 BM25 召回，或者各类Sentence Transformer的稠密召回向量
3. 精排阶段。排序指标：MAP（mean average precision）、MRR(mean reciprocal rank)、nDCG
4. 答案生成阶段

``` python
# nDGG
import numpy as np

def ndcg(relevance_scores, ideal_scores, k=None):
    """计算nDCG@k"""
    k = k or len(relevance_scores)
    # 计算DCG@k
    dcg = sum(rel / np.log2(i + 2) for i, rel in enumerate(relevance_scores[:k]))
    # 计算IDCG@k
    idcg = sum(rel / np.log2(i + 2) for i, rel in enumerate(ideal_scores[:k]))
    return dcg / idcg if idcg > 0 else 0.0

# RAG检索结果示例
retrieved_scores = [3, 2, 3, 0, 1]  # 实际返回文档的相关性
ideal_scores = sorted([3, 2, 3, 0, 1], reverse=True)  # 理想排序
print(f"nDCG@5: {ndcg(retrieved_scores, ideal_scores, k=5):.3f}")
```

| **指标**                             | 公式                                | 说明                                                  |
| ------------------------------------ | ----------------------------------- | ----------------------------------------------------- |
| **CG（累计增益）**                   | CG@k=∑i=1kreli                      | 前k个文档的相关性总分，忽略排序位置的影响。           |
| **DCG（折损累计增益）**              | DCG@k=∑i=1klog2(i+1)reli            | 对排名靠后的文档施加对数衰减，强调前排文档的重要性。  |
| **IDCG（理想DCG）**                  | IDCG@k=∑i=1klog2(i+1)rel理想排序[i] | 将文档按真实相关性降序排列后计算的DCG，是理论最大值。 |
| **nDCG（归一化DCG）**                | nDCG@k=IDCG@kDCG@k                  | 将DCG归一化到[0,1]，1表示完美排序，0表示最差排序。    |
| ![[Pasted image 20250725083805.png]] |                                     |                                                       |
## 0.10 大模型智能体

# 1 重点
## 1.1 检索增强生成
## 1.2 大模型智能体
## 1.3 大模型PEFT
## 1.4 大模型的训练与推理
## 1.5 DeepSeek
# 2 八股问题
以下问题可结合Karpathy的视频写答案
## 2.1 语义表达
词向量如何建模语义信息？稀疏词向量和稠密词向量有什么区别？
在构建词向量的过程中，怎么处理溢出词表词问题？
词 / 子词 / 字符粒度的分词方法对构建词向量有何影响？
如何利用词向量进行无监督句子相似度计算任务？
如何使用BERT构建有聚类性质的句子向量？
基于Transformer的与训练语言模型如何区分文本位置？
为什么在BERT的输入层中3种嵌入表达要相加？
大模型的隐含语义是如何建模的？有哪几种典型架构？
## 2.2 大模型的数据
用来训练大模型的开源数据集有哪些？
主流开源大模型所用的训练数据量如何？各个环节的数据量如何？
大模型数据预处理流程要注意哪些核心要点？
大模型中的扩展法则是什么？如何推演？
持续与训练有什么作用？如何缓解大模型微调后的通用能力遗忘问题？
大模型指令微调有哪些筛选数据的方法？
## 2.3 大模型的预训练
预训练和监督微调有什么区别和相同之处？
大模型的涌现能力指什么？
大模型在与训练阶段有哪些提效实验和保障稳定性的方法？
大模型预训练、监督微调和强化学习分别解决什么问题？有什么必要性?
大模型训练过程中如何计算显存？优化显存占用的方法有哪些？
大模型训练过程中的通信开销如何计算？
## 2.4 大模型的对齐
大模型对齐训练需要什么样的数据？如何高效的构造这些数据？
什么是PPO？他有什么特点？
决定奖励模型训练质量的关键因素有哪些？
提升PPO训练稳定性的方法有哪些？
DPO算法主要解决什么问题？具体理论依据和实现逻辑是什么？
对比DPO和PPO，二者各有什么特点？
除了PPO和DPO，还有那些进行偏好对齐的算法？他们各是怎样进行优化的？
如何有效监控对齐训练过程中的大模型表现？
监督微调阶段的对齐和RLHF阶段的对齐有何异同？
## 2.5 大模型的垂类微调
在进行垂类下游任务微调时，通常是选择基座模型还是聊天模型？如何构造多轮对话的输入格式？
对大模型进行词表扩充是否有必要？他对模型的训练效果有什么影响？可以用哪些方法评估词表的效率？
提升大模型长度外推性能的方法有哪些？
大模型微调时可以使用那些损失函数，它们的原理是什么？有何特点？
如果希望将大模型用于知识密集型场景问答，并且这些场景中的知识可能会发生频繁的更新，那么在这种情况下有哪些解决方案？
## 2.6 大模型的组件
Transformer的结构和工作原理是什么？
在Transformer中计算注意力分数时为什么需要除以常数项？
现有的词元化算法都有哪些？他们有何特点？
RoPE的工作原理是什么？
ALiBi的工作原理是什么？它的外推能力如何？
Sparse Attention是什么？有何特点？
Linear Attention是什么？有何特点？
多头注意力机制如何用代码实现？多查询注意力和分组查询注意力的工作原理是什么？它们有何特点？
Batch Norm、LayerNorm和RMSNorm的工作原理各是什么？三者有何特点？
PostNorm和PreNorm的联系和区别分别是什么？
Dropout的工作原理是什么？怎么避免训练推理的期望和方差出现便宜？
初始化模型参数的常见方法有哪些？他们有何特点？
## 2.7 大模型的评估
主流的大模型评测排行榜有哪些？具体的评测形式如何？现阶段的大模型评测排行榜存在哪些问题？
大模型评测要关注那些原则？
大模型如何修复badcase?
生成式任务的经典指标有哪些？如何计算？
如何利用自动化测试工具评估大模型的性能？
如何设计大模型的对抗性测试来确保其稳健性？
大模型风控合规和安全考量在开发中要如何实践？大模型备案有哪些流程，需要哪些资料？
## 2.8 大模型的架构
为什么现在的大模型大多采用因果解码器架构？有什么优势？
大模型的集成融合有哪些方法？这些方法分别对架构有何改进？
MoE训练与一般的大模型有何区别？在推理速度和模型的参数量上怎么预估？
## 2.9 检索增强生成
大模型中的RAG链路有哪些基本模块？如何评估各个模块的效果？
RAG中的召回方法有哪些？
RAG召回后，生成前阶段都做了哪些工作？
在RAG工程化阶段可能会遇到哪些问题？
## 2.10 大模型智能体
大模型智能体由哪些基本模块构成？
大模型智能体的规划能力有哪些提升方法？
大模型智能体的记忆模块在哪些方面可以优化？
大模型智能体的工具调用能力是什么？Tool LLM有那些针对性的提升点？
XAgent框架的基本原理是什么？
AutoGen框架的基本原理和特点是什么？
结合使用GPT-4和代码解释器，构造一个交互式编写代码的示例程序（demo），完成从百度首页下载logo的任务
## 2.11 大模型PEFT
LoRA的基本原理是什么？它的具体实现流程可以分为几步？
除了LoRA，你还知道NLP任务中的哪些PEFT方法？
PEFT与全参数微调该如何选型？
## 2.12 大模型的训练与推理
生成式模型的解码与采样方法有哪些？
大模型生成函数generate中各个超参数的含义及其作用是什么？
Flash Attention的优化方法有哪些？他如何实现数学等价性？
MoE并行训练中的专家并行是什么？
vLLM是什么？其背后的PagedAttention原理是什么？
为什么有些框架在直接使用PyTorch运行量化模型时速度会变得更慢？
数据并行、张量并行和流水并行的工作原理分别是什么？它们的最佳组合有哪些？
## 2.13 DeepSeek
DeepSeek系列大语言模型在模型架构上的创新点有哪些？
DeepSeek-R1的训练流程是怎样的？