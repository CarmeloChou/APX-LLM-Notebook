# Inference Review

## 一、核心概念

**大模型推理**（Inference）是指训练完成的大规模语言模型接收输入（Prompt），经过计算生成输出结果的过程。与训练阶段调整模型参数不同，推理阶段**模型参数固定**，核心目标是**高效、稳定、高质量地生成文本**。

推理流程可分为三个核心阶段：

- **输入预处理**：将原始文本通过分词工具（如SentencePiece、BPE）切分为离散token序列
- **模型执行**：神经网络前向传播与大规模矩阵运算，涉及Transformer架构的自注意力机制
- **输出解码**：采用自回归生成机制，通过Key-Value Cache（KV Cache）缓存已生成token的Key和Value矩阵，避免重复计算

## 二、解码策略详解

### 1. 贪婪解码（Greedy Search）

**核心机制**：每一步选择概率最高的token

- **优点**：计算效率高、实现简单、内存占用极低、结果确定可复现
- **缺点**：容易陷入局部最优，生成文本重复性高、缺乏多样性
- **适用场景**：短文本生成、对响应时间要求高的场景

### 2. 束搜索（Beam Search）

**核心机制**：维护多个候选序列（束宽k），选择整体概率最高的序列

- **工作流程**：初始化→迭代拓展→评分筛选→终止判断
- **优点**：在探索与利用间取得平衡，相比贪婪解码产生更多样化文本
- **缺点**：计算成本随束宽增加显著上升，仍可能出现重复问题
- **典型设置**：束宽通常为3-6

### 3. 采样策略

| 策略                    | 核心机制                               | 优点                                         | 缺点                   |
| ----------------------- | -------------------------------------- | -------------------------------------------- | ---------------------- |
| **Top-k采样**           | 从概率最高的前k个token中采样           | 限制候选词数量，避免极端词汇选择             | k值固定，无法动态调整  |
| **Top-p采样**（核采样） | 累积概率达到阈值p的最小token集合中采样 | 动态调整候选池大小，更智能平衡多样性/质量    | p值选择需精细调优      |
| **温度采样**            | 通过温度参数调整概率分布形状           | 控制输出随机性，温度低更保守、温度高更创造性 | 过高温度可能导致不连贯 |
| **Min-p采样**           | 根据模型置信度动态调整采样阈值         | 在高温度下保持连贯性，平衡创造性与连贯性     | 计算效率有待优化       |

**参数设置建议**：

- 温度：0.7-1.0（创意写作可调高，代码生成需调低）
- Top-p：0.8-0.95
- Top-k：40-100

## 三、效率优化技术

### 1. 量化技术

**核心原理**：将高精度参数（FP32）转换为低精度格式（INT8/INT4）

- **训练后量化（PTQ）**：训练后直接量化，操作简单但精度损失较大
- **量化感知训练（QAT）**：训练时模拟量化误差，精度损失小
- **效果**：FP32→INT8可减少75%存储量，计算速度提升2-4倍

### 2. 剪枝技术

**核心原理**：移除冗余参数或神经元

- **非结构化剪枝**：移除单个冗余权重，压缩率高（50%-90%）但硬件加速困难
- **结构化剪枝**：按结构单元（通道、层）移除，适配硬件加速，压缩率30%-60%
- **效果**：减少40%-60%计算量，模型体积缩小30%-50%

### 3. 知识蒸馏

**核心原理**：用大模型（教师）指导小模型（学生）训练

- **软标签蒸馏**：使用教师模型的概率分布作为学习目标
- **特征蒸馏**：对齐中间层特征
- **效果**：学生模型体积缩小10-100倍，性能接近教师模型

### 4. 内存管理优化

- **PagedAttention**：借鉴操作系统分页机制，将KV Cache分割为固定大小块，显存利用率从<40%提升至95%+
- **KV量化**：采用INT4/FP8低精度格式存储KV缓存，内存占用降低75%
- **动态批处理**：实时将新请求加入处理队列，GPU利用率提升30%+

## 四、推理引擎对比

| 引擎             | 核心特性                                                | 适用场景                    |
| ---------------- | ------------------------------------------------------- | --------------------------- |
| ==**vLLM**==     | PagedAttention技术，显存利用率95%+，吞吐量提升14-24倍   | 高并发API服务、长上下文推理 |
| **TensorRT-LLM** | 深度硬件绑定NVIDIA GPU，支持FP8/INT4量化，低延迟        | 实时对话系统、延迟敏感场景  |
| **SGLang**       | RadixAttention缓存机制，多轮对话前缀复用，吞吐量提升5倍 | 交互式对话、客服助手        |
| **TGI**          | 生态兼容性好，支持多平台硬件，部署门槛低                | 中小企业快速集成            |